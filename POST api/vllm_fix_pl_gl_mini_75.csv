,model,PL,GL,CL,batchsize,MQTS,CPL,mode_type,precision,hf-source,status,E2E_processed(tok/sec),E2E_generated(tok/sec),row,compile-only,time-passes,vvv,aic-perf-warnings,version-extended,aic-hw-version,aic-hw,retained-state,mxfp6-matmul,allow-mxint8-mdp-io,m,tokenizer_path,network-specialization-config,custom-IO-list-file,model-source,compile-app,execute-app,customer,model-framework,threads-per-queue,tag,compile_timeout,execute_timeout,server_timeout,e2e_lat_timeout,mos,ols,convert-to-fp16,aic-enable-depth-first,register-custom-op,beam_width,ignore-eos,derated_ttft(sec),derated_decode_tpt_per_card(tok/sec),derated_e2e_tpt_per_card(tok/sec),aic-num-cores,aic-binary-dir,mdp-load-partition-config,device-id,compilation-log-path,execution-log-path,crash-dump-path,qdss-log-path,skip_keys,failure_cause,compilation_command,quantization,temperature,kv-cache-dtype,backend,trust-remote-code,seed,device,execution_command,instances,max_dram_total_KB,median_dram_total_KB,max_dram_free_KB,median_dram_free_KB,max_dram_utilization_%,median_dram_utilization_%,max_dram_bw_KBps,median_dram_bw_KBps,max_soc_power_watts,median_soc_power_watts,max_board_power_watts,median_board_power_watts,max_soc_temparature_degree_C,median_soc_temparature_degree_C,QPC_size,time,Average_TTFT(sec),model_load_time(sec),compile_time,infer_status
0,Llama3.1-8B,128,128,256,1,4,128,P2P,mx6,meta-llama/Llama-3.1-8B-Instruct,PASS,100.26,50.13,18,True,True,True,True,True,2.0,True,True,True,True,/home/qraniumtest/QEFF_MODELS/qeff_cache/meta-llama/Meta-Llama-3.1-8B-Instruct/onnx_meta_llama_Meta_Llama_3.1_8B_Instruct_with_fbs/meta-llama_Meta-Llama-3.1-8B-Instruct_kv.onnx,meta-llama/Llama-3.1-8B-Instruct,/home/qraniumtest/AIC.1.20.0.100/Cloud_LLM_performance/2574/network-specialization-config/Llama3.1-8B/1_128_256.json,/home/qraniumtest/QEFF_MODELS/qeff_cache/meta-llama/Meta-Llama-3.1-8B-Instruct/onnx_meta_llama_Meta_Llama_3.1_8B_Instruct_with_fbs/custom_io_int8.yaml,,/opt/qti-aic/exec/qaic-exec,/home/qraniumtest/AIC.1.20.0.100/Cloud_LLM_performance/2574/qserve/qserve/benchmarks/benchmark_throughput.py,AWS,onnx,4,"cloud,LLM-KV-cache,1QPC,qnn_release",604800,604800,1800,604800,,,True,True,,,True,,,,16,/home/qraniumtest/AIC.1.20.0.100/Cloud_LLM_performance/2574/Binaries/Llama3.1-8B/mx6/P2P/4/bs_1_cpl_128_cl_256,/home/qraniumtest/AIC.1.20.0.100/Cloud_LLM_performance/2574/MDP-parition-file/p2p-mq-config-4.json,"['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15']",/home/qraniumtest/AIC.1.20.0.100/Cloud_LLM_performance/2574/Logs/Llama3.1-8B/mx6/P2P/4/bs_1_cpl_128_cl_256/compilation_log.txt,/home/qraniumtest/AIC.1.20.0.100/Cloud_LLM_performance/2574/Logs/Llama3.1-8B/mx6/P2P/4/bs_1_cpl_128_cl_256/prompt_len_128,/home/qraniumtest/AIC.1.20.0.100/Cloud_LLM_performance/2574/Qaic_crashdump/Llama3.1-8B/mx6/P2P/4/bs_1_cpl_128_cl_256/prompt_len_128,/home/qraniumtest/AIC.1.20.0.100/Cloud_LLM_performance/2574/QDSS_logs/Llama3.1-8B/mx6/P2P/4/bs_1_cpl_128_cl_256/prompt_len_128,[],,/opt/qti-aic/exec/qaic-exec -aic-binary-dir=/home/qraniumtest/AIC.1.20.0.100/Cloud_LLM_performance/2574/Binaries/Llama3.1-8B/mx6/P2P/4/bs_1_cpl_128_cl_256 -custom-IO-list-file=/home/qraniumtest/QEFF_MODELS/qeff_cache/meta-llama/Meta-Llama-3.1-8B-Instruct/onnx_meta_llama_Meta_Llama_3.1_8B_Instruct_with_fbs/custom_io_int8.yaml -m=/home/qraniumtest/QEFF_MODELS/qeff_cache/meta-llama/Meta-Llama-3.1-8B-Instruct/onnx_meta_llama_Meta_Llama_3.1_8B_Instruct_with_fbs/meta-llama_Meta-Llama-3.1-8B-Instruct_kv.onnx -allow-mxint8-mdp-io -time-passes -convert-to-fp16 -aic-hw -compile-only -aic-hw-version=2.0 -aic-perf-warnings -retained-state -version-extended -vvv -aic-num-cores=16 -network-specialization-config=/home/qraniumtest/AIC.1.20.0.100/Cloud_LLM_performance/2574/network-specialization-config/Llama3.1-8B/1_128_256.json -mxfp6-matmul -mdp-load-partition-config=/home/qraniumtest/AIC.1.20.0.100/Cloud_LLM_performance/2574/MDP-parition-file/p2p-mq-config-4.json -aic-enable-depth-first,mxfp6,0.0,mxint8,vllm,True,20,qaic,python3 /home/qraniumtest/AIC.1.20.0.100/Cloud_LLM_performance/2574/qserve/qserve/benchmarks/benchmark_throughput.py --kv-cache-dtype mxint8 --max-seq-len-to-capture 128 --fixed-gen-len 128 --temperature 0.0 --trust-remote-code --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --device qaic --num-prompts 1 --max-num-seqs 1 --max-model-len 256 --quantization mxfp6 --fixed-input-len 128 --seed 20,1,33488896,33488896.0,33235535,33235535.0,12.37,0.76,22654912,155184.0,15,10.0,49,45.0,44,41.5,13.38,00:00:58,0.06,12.0, 00:14:38,True
